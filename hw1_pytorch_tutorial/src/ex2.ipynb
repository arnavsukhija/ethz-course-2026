{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfac88e5",
   "metadata": {},
   "source": [
    "# Exercise 2: PyTorch core\n",
    "\n",
    "In this exercise you’ll build core PyTorch “muscle memory” that you’ll reuse in basically every model you write:\n",
    "\n",
    "- **Autograd**: how gradients are created, how they accumulate, and how to compute gradients for one or multiple inputs.\n",
    "- **Dataloading**: writing small `Dataset`s, using `DataLoader`, and custom `collate_fn`.\n",
    "- **Optimizers**: implementing **AdamW** updates from scratch (state, bias correction, weight decay).\n",
    "- **Training basics**: a clean single training step.\n",
    "- **Initialization**: fan-in/out and common initializers (Xavier / Kaiming), plus a helper to init `nn.Linear`.\n",
    "\n",
    "As before: fill in all `TODO`s without changing function names or signatures.\n",
    "When debugging, print shapes/dtypes/devices, and write tiny sanity checks (e.g. compare to PyTorch’s built-ins).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0145b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T09:58:12.102011Z",
     "start_time": "2026-02-23T09:58:12.076419Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "419608fc",
   "metadata": {},
   "source": [
    "## Autograd fundamentals\n",
    "\n",
    "PyTorch builds a computation graph when you apply operations to tensors with `requires_grad=True`.\n",
    "Calling `backward()` (or `torch.autograd.grad`) computes gradients by traversing that graph.\n",
    "\n",
    "### Key concepts\n",
    "- **Leaf tensor**: a tensor created by you (not the result of an operation) with `requires_grad=True`. Leaf tensors can store gradients in `.grad`.\n",
    "- **Gradient accumulation**: calling `backward()` adds into `.grad` (it does not overwrite). You must reset gradients between steps/calls.\n",
    "- **`torch.autograd.grad` vs `.backward()`**\n",
    "  - `torch.autograd.grad(f, x)` returns `df/dx` directly and does not write into `x.grad` unless you explicitly do so.\n",
    "  - `f.backward()` writes gradients into `.grad` of leaf tensors.\n",
    "\n",
    "In the next functions you’ll compute gradients for a simple scalar function such as `f(x) = sum(x^2)` using both APIs.\n",
    "\n",
    "### `torch.no_grad()`\n",
    "Wrap inference-only code to avoid tracking gradients and building graphs:\n",
    "- saves memory\n",
    "- speeds up evaluation\n",
    "\n",
    "### `detach()`\n",
    "`y = x.detach()` returns a tensor that shares data with `x` but is **not connected** to the autograd graph.\n",
    "This is useful when you want to treat something as a constant target.\n",
    "\n",
    "### `model.train()` vs `model.eval()`\n",
    "- `train()` enables training behavior (e.g. dropout active, batchnorm updates running stats).\n",
    "- `eval()` enables inference behavior (e.g. dropout off, batchnorm uses running stats)."
   ]
  },
  {
   "cell_type": "code",
   "id": "0b7724fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T10:10:33.338490Z",
     "start_time": "2026-02-23T10:10:33.181935Z"
    }
   },
   "source": [
    "def grad_with_autograd_grad(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute gradient of f(x) = sum(x^2) using torch.autograd.grad\n",
    "\n",
    "    Requirements:\n",
    "    - Do not call .backward().\n",
    "    - x should require grad inside the function (don't assume it does).\n",
    "    - Must return df/dx\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    x.requires_grad_(True)\n",
    "    return torch.autograd.grad(torch.sum(x**2), x)[0]\n",
    "\n",
    "x = torch.ones(5)\n",
    "grad = grad_with_autograd_grad(x)\n",
    "print(grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "ea0d32cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T10:15:45.361513Z",
     "start_time": "2026-02-23T10:15:45.303094Z"
    }
   },
   "source": [
    "\n",
    "def grad_with_backward(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute gradient of f(x) = sum(x^2) using .backward().\n",
    "\n",
    "    Requirements:\n",
    "    - Must return df/dx\n",
    "    - Must not leak gradients across calls (watch x.grad accumulation)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    x_local = x.detach().requires_grad_(True)\n",
    "    y = torch.sum(x_local ** 2)\n",
    "    torch.autograd.backward(y)\n",
    "    return x_local.grad\n",
    "\n",
    "x = torch.ones(5)\n",
    "grad = grad_with_backward(x)\n",
    "print(grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "bec40995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T10:20:58.422484Z",
     "start_time": "2026-02-23T10:20:58.417241Z"
    }
   },
   "source": [
    "def grad_wrt_multiple_inputs(\n",
    "    a: torch.Tensor, b: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute gradients w.r.t. multiple inputs. The function is f(a, b) = sum(a^2 + ab).\n",
    "\n",
    "    Return:\n",
    "        (df/da, df/db)\n",
    "\n",
    "    Requirements:\n",
    "    - Use torch.autograd.grad\n",
    "    - Ensure both a and b require grad in this function.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    a.requires_grad_(True)\n",
    "    b.requires_grad_(True)\n",
    "    f = torch.sum(a ** 2 + a * b)\n",
    "    grad = torch.autograd.grad(f, [a,b])\n",
    "    return grad\n",
    "\n",
    "a = torch.tensor([1.0])\n",
    "b = torch.tensor([2.0])\n",
    "grad_a, grad_b = grad_wrt_multiple_inputs(a, b)\n",
    "print(grad_a)\n",
    "print(grad_b)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.])\n",
      "tensor([1.])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "899a026b",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "\n",
    "In PyTorch, a `Dataset` defines how to fetch a *single* training example, and a `DataLoader` handles:\n",
    "- batching\n",
    "- shuffling\n",
    "- parallel workers\n",
    "- optional custom batching logic via `collate_fn`\n",
    "\n",
    "### `Dataset` in one sentence\n",
    "A `Dataset` only needs:\n",
    "- `__len__`: number of items\n",
    "- `__getitem__`: return one item (e.g. `(x, y)`)\n",
    "\n",
    "### Why `collate_fn` matters\n",
    "The default DataLoader collation stacks items along a new batch dimension.\n",
    "That works for fixed-size tensors, but it breaks for **variable-length sequences**.\n",
    "\n",
    "So we’ll implement padding ourselves:\n",
    "- Convert a list of 1D token sequences into a padded tensor `(B, T_max)`\n",
    "- Track `lengths` and a `padding_mask`\n",
    "\n",
    "### Mask convention for padding\n",
    "For padding masks in this exercise:\n",
    "- `padding_mask[b, t] == True` means **this is padding / invalid**\n",
    "- `padding_mask[b, t] == False` means **this is a real token**"
   ]
  },
  {
   "cell_type": "code",
   "id": "3dbd7a6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T10:26:26.258661Z",
     "start_time": "2026-02-23T10:26:26.255379Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "09ab0d8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T10:32:20.917528Z",
     "start_time": "2026-02-23T10:32:20.894487Z"
    }
   },
   "source": [
    "class TensorPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Minimal dataset wrapping (x, y).\n",
    "\n",
    "    x: (N, ...)\n",
    "    y: (N, ...)\n",
    "\n",
    "    N is the number of samples. The dataset should return tuples of (x[i], y[i]).\n",
    "    \"\"\"\n",
    "    data: tuple[torch.Tensor, torch.Tensor]\n",
    "\n",
    "    def __init__(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        # TODO: implement\n",
    "        assert len(x) == len(y)\n",
    "        self.data = x, y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # TODO: implement\n",
    "        return len(self.data[0])\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # TODO: implement\n",
    "        return self.data[0][idx], self.data[1][idx]\n",
    "    \n",
    "x_test = torch.randn(5,10)\n",
    "y_test = torch.arange(5).view(5,1)\n",
    "dataset = TensorPairDataset(x_test, y_test)\n",
    "print(f\"Dataset has length {len(dataset)}\")\n",
    "x_sample, y_sample = dataset[2]\n",
    "print(f\"Sample at index 2, y = {y_sample}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has length 5\n",
      "Sample at index 2, y = tensor([2])\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "e71e02fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T10:42:33.132885Z",
     "start_time": "2026-02-23T10:42:33.096621Z"
    }
   },
   "source": [
    "class NextTokenDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Next-token prediction dataset.\n",
    "\n",
    "    Given tokens of shape (N, T), produce:\n",
    "      input_ids  = tokens[:, :-1]\n",
    "      target_ids = tokens[:, 1:]\n",
    "\n",
    "    Return per item:\n",
    "      (input_ids, target_ids)\n",
    "\n",
    "    Notes:\n",
    "    - Returned tensors should be 1D of length (T-1).\n",
    "    - dtype should remain integer.\n",
    "    \"\"\"\n",
    "    tokens: torch.Tensor\n",
    "\n",
    "    def __init__(self, tokens: torch.Tensor):\n",
    "        # TODO: implement\n",
    "        self.tokens = tokens\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # TODO: implement\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # TODO: implement\n",
    "        sequence = self.tokens[idx]\n",
    "        input_ids = sequence[:-1]\n",
    "        target_ids = sequence[1:]\n",
    "        return input_ids, target_ids\n",
    "    \n",
    "test_tokens = torch.tensor([\n",
    "    [10, 20, 30, 40, 50],\n",
    "    [11, 21, 31, 41, 51]\n",
    "])\n",
    "\n",
    "dataset = NextTokenDataset(test_tokens)\n",
    "\n",
    "inputs, targets = dataset[0]\n",
    "print(inputs)\n",
    "print(targets)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 20, 30, 40])\n",
      "tensor([20, 30, 40, 50])\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "43d5d3e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T10:51:21.902131Z",
     "start_time": "2026-02-23T10:51:21.894187Z"
    }
   },
   "source": [
    "\n",
    "class RandomCropSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Sequence dataset that returns random crops of fixed length.\n",
    "\n",
    "    tokens: (N, T_total)\n",
    "    crop_len: L\n",
    "\n",
    "    For each __getitem__:\n",
    "      - sample a start index s so that s+L <= T_total\n",
    "      - return tokens[idx, s:s+L]\n",
    "\n",
    "    Requirements:\n",
    "    - Use a torch.Generator for deterministic behavior if seed is provided.\n",
    "    - Do NOT use Python's random module.\n",
    "    \"\"\"\n",
    "    tokens: torch.Tensor\n",
    "    crop_len: int\n",
    "    seed: int | None\n",
    "\n",
    "    def __init__(self, tokens: torch.Tensor, crop_len: int, seed: int | None = None):\n",
    "        # TODO: implement\n",
    "        self.tokens = tokens\n",
    "        self.crop_len = crop_len\n",
    "        self.generator = torch.Generator()\n",
    "        self.T_total = self.tokens.size(1)\n",
    "        if seed is not None:\n",
    "            self.generator.manual_seed(seed)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # TODO: implement\n",
    "        return self.tokens.size(0)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        # TODO: implement\n",
    "        max_s = self.T_total - self.crop_len\n",
    "        s = torch.randint(0, max_s + 1, (1,), generator=self.generator).item()\n",
    "        return self.tokens[idx, s:s+self.crop_len]\n",
    "    \n",
    "test_tokens = torch.arange(100).view(1, 100)\n",
    "L = 10 \n",
    "dataset = RandomCropSequenceDataset(test_tokens, crop_len=L, seed=42)\n",
    "crop1 = dataset[0]\n",
    "crop2 = dataset[0]\n",
    "print(f\"Crop 1: {crop1}\")\n",
    "print(f\"Crop 2: {crop2}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop 1: tensor([57, 58, 59, 60, 61, 62, 63, 64, 65, 66])\n",
      "Crop 2: tensor([86, 87, 88, 89, 90, 91, 92, 93, 94, 95])\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "73593f16",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T11:00:21.096766Z",
     "start_time": "2026-02-23T11:00:21.067824Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PaddedBatch:\n",
    "    \"\"\"\n",
    "    A padded batch for variable-length sequences.\n",
    "\n",
    "    tokens: LongTensor (B, T_max)\n",
    "    lengths: LongTensor (B,)\n",
    "    padding_mask: BoolTensor (B, T_max) where True means \"this is padding\"\n",
    "    \"\"\"\n",
    "\n",
    "    tokens: torch.Tensor\n",
    "    lengths: torch.Tensor\n",
    "    padding_mask: torch.Tensor\n",
    "\n",
    "\n",
    "def pad_1d_sequences(seqs: list[torch.Tensor], pad_value: int = 0) -> PaddedBatch:\n",
    "    \"\"\"\n",
    "    Pad a list of 1D integer tensors to the same length.\n",
    "\n",
    "    Requirements:\n",
    "    - Return PaddedBatch(tokens, lengths, padding_mask)\n",
    "    - padding_mask[b, t] == True iff t >= lengths[b]\n",
    "    - tokens should be dtype long, if not cast them\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    B = len(seqs)\n",
    "    lengths = torch.tensor([len(s) for s in seqs], dtype=torch.long)\n",
    "    T_max = lengths.max().item() if B > 0 else 0\n",
    "    \n",
    "    tokens = torch.full((B, T_max), pad_value, dtype=torch.long)\n",
    "    \n",
    "    for i, seq in enumerate(seqs):\n",
    "        tokens[i, :len(seq)] = seq.long()\n",
    "    \n",
    "    arrange_t = torch.arange(T_max).expand(B, T_max)\n",
    "    padding_mask = arrange_t >= lengths.unsqueeze(1)\n",
    "    \n",
    "    return PaddedBatch(tokens=tokens, lengths=lengths, padding_mask=padding_mask)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "e2df849a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T11:19:07.660365Z",
     "start_time": "2026-02-23T11:19:07.652251Z"
    }
   },
   "source": [
    "def collate_next_token_batch(\n",
    "    batch: list[tuple[torch.Tensor, torch.Tensor]], pad_value: int = 0\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Collate for NextTokenDataset samples that may have variable lengths.\n",
    "\n",
    "    batch: list of (input_ids, target_ids), each 1D\n",
    "\n",
    "    Return dict with:\n",
    "      - input_ids: (B, T_max)\n",
    "      - target_ids: (B, T_max)\n",
    "      - attention_mask: (B, T_max) where True means \"keep\" (NOT padding)\n",
    "      - padding_mask: (B, T_max) where True means \"padding\"\n",
    "\n",
    "    Requirements:\n",
    "    - pad input_ids and target_ids consistently\n",
    "    - attention_mask is the logical NOT of padding_mask\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    inputs, targets = zip(*batch)\n",
    "    padded_lists = pad_1d_sequences(list(inputs)+list(targets), pad_value=pad_value)\n",
    "    B = len(inputs)\n",
    "    input_ids = padded_lists.tokens[:B]\n",
    "    target_ids = padded_lists.tokens[B:]\n",
    "    \n",
    "    padding_mask = padded_lists.padding_mask[:B]\n",
    "    attention_mask = ~padding_mask\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'target_ids': target_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'padding_mask': padding_mask,\n",
    "    }"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "904bdd57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T11:20:05.260594Z",
     "start_time": "2026-02-23T11:20:05.257055Z"
    }
   },
   "source": [
    "def make_dataloader(\n",
    "    dataset: Dataset,\n",
    "    batch_size: int,\n",
    "    shuffle: bool = True,\n",
    "    drop_last: bool = False,\n",
    "    collate_fn=None,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a DataLoader with optional collate_fn.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    return DataLoader(dataset, batch_size, shuffle, drop_last=drop_last, collate_fn=collate_fn, num_workers=num_workers)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "d8ab920b",
   "metadata": {},
   "source": [
    "## Optimizers (AdamW from scratch)\n",
    "\n",
    "PyTorch optimizers keep **state** for each parameter (e.g. moment estimates in Adam).\n",
    "In this section you’ll implement **AdamW**, which is Adam + *decoupled* weight decay.\n",
    "\n",
    "### AdamW state\n",
    "For each parameter tensor `p` we store:\n",
    "- `m`: first moment (EMA of gradients)\n",
    "- `v`: second moment (EMA of squared gradients)\n",
    "- `t`: step counter\n",
    "\n",
    "### Update overview (high level)\n",
    "1) Update moments `m, v`\n",
    "2) Bias-correct them (`m_hat, v_hat`)\n",
    "3) Apply parameter update:\n",
    "   `p -= lr * ( m_hat / (sqrt(v_hat) + eps) + weight_decay * p )`\n",
    "\n",
    "Notes:\n",
    "- This update is **in-place** (mutates `p`).\n",
    "- Gradients should not be modified.\n",
    "- State tensors must match parameter shape/device/dtype."
   ]
  },
  {
   "cell_type": "code",
   "id": "be4c124c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T11:24:50.268580Z",
     "start_time": "2026-02-23T11:24:50.260514Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class AdamWState:\n",
    "    \"\"\"\n",
    "    Per-parameter AdamW state.\n",
    "\n",
    "    m: first moment\n",
    "    v: second moment\n",
    "    t: step count\n",
    "    \"\"\"\n",
    "\n",
    "    m: torch.Tensor\n",
    "    v: torch.Tensor\n",
    "    t: int\n",
    "\n",
    "\n",
    "def init_adamw_state(p: torch.Tensor) -> AdamWState:\n",
    "    \"\"\"\n",
    "    Initialize AdamW state tensors for a parameter tensor p.\n",
    "\n",
    "    What to create:\n",
    "    - m: zeros like p, same shape/device/dtype\n",
    "    - v: zeros like p, same shape/device/dtype\n",
    "    - t: step counter starting at 0\n",
    "\n",
    "    Notes / requirements:\n",
    "    - Use torch.zeros_like(p) for m and v.\n",
    "    - Do NOT attach gradients to the state (initialize under torch.no_grad()).\n",
    "    - t starts at 0. In adamw_step_, increment t to 1 on the first update *before*\n",
    "      computing bias correction terms (1 - beta1^t) and (1 - beta2^t).\n",
    "    - State tensors must live on the same device as p (CPU vs GPU) and have the\n",
    "      same dtype as p.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    with torch.no_grad():\n",
    "        m = torch.zeros_like(p)\n",
    "        v = torch.zeros_like(p)\n",
    "        t = 0\n",
    "    return AdamWState(m, v, t)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "d7eeaa78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T11:40:30.063382Z",
     "start_time": "2026-02-23T11:40:30.043864Z"
    }
   },
   "source": [
    "def adamw_step_(\n",
    "    p: torch.Tensor,\n",
    "    grad: torch.Tensor,\n",
    "    state: AdamWState,\n",
    "    lr: float,\n",
    "    betas: tuple[float, float] = (0.9, 0.999),\n",
    "    eps: float = 1e-8,\n",
    "    weight_decay: float = 0.01,\n",
    ") -> AdamWState:\n",
    "    \"\"\"\n",
    "    In-place AdamW parameter update (updates p).\n",
    "\n",
    "    Algorithm (AdamW):\n",
    "      m = beta1*m + (1-beta1)*grad\n",
    "      v = beta2*v + (1-beta2)*grad^2\n",
    "      m_hat = m / (1 - beta1^t)\n",
    "      v_hat = v / (1 - beta2^t)\n",
    "      p = p - lr * (m_hat / (sqrt(v_hat) + eps) + weight_decay * p)\n",
    "\n",
    "    Requirements:\n",
    "    - Update p in-place.\n",
    "    - Return updated state (with incremented t).\n",
    "    - Do not modify grad.\n",
    "    - Should work for any tensor shape.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    state.t += 1\n",
    "    state.m.mul_(betas[0]).add_(grad, alpha=1 - betas[0])\n",
    "    state.v.mul_(betas[1]).addcmul_(grad, grad, value=1 - betas[1])\n",
    "    m_hat = state.m / (1 - betas[0]**state.t)\n",
    "    v_hat = state.v / (1 - betas[1]**state.t)\n",
    "    p.mul_(1 - lr * weight_decay)\n",
    "    update = m_hat / (torch.sqrt(v_hat) + eps)\n",
    "    p.add_(update, alpha = -lr)\n",
    "    return state"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "9bf5bd92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T11:46:48.062198Z",
     "start_time": "2026-02-23T11:46:48.054483Z"
    }
   },
   "source": [
    "def adamw_step_many_(\n",
    "    params: list[torch.Tensor],\n",
    "    grads: list[torch.Tensor],\n",
    "    states: list[AdamWState],\n",
    "    lr: float,\n",
    "    betas: tuple[float, float] = (0.9, 0.999),\n",
    "    eps: float = 1e-8,\n",
    "    weight_decay: float = 0.01,\n",
    ") -> list[AdamWState]:\n",
    "    \"\"\"\n",
    "    Apply AdamW to many parameters.\n",
    "\n",
    "    Requirements:\n",
    "    - len(params) == len(grads) == len(states)\n",
    "    - Update each param in-place.\n",
    "    - Return the list of updated states.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    updated_states = []\n",
    "    for p,g,s in zip(params, grads, states):\n",
    "        updated_state = adamw_step_(p,g,s,lr,betas,eps,weight_decay)\n",
    "        updated_states.append(updated_state)\n",
    "    \n",
    "    return updated_states"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "81b571bd",
   "metadata": {},
   "source": [
    "## Training basics\n",
    "\n",
    "A minimal training step follows the same pattern almost everywhere:\n",
    "\n",
    "1) set model to train mode\n",
    "2) reset gradients\n",
    "3) forward pass\n",
    "4) compute loss\n",
    "5) backward pass\n",
    "6) step optimizer\n",
    "\n",
    "In this exercise you’ll implement a single MSE training step using a standard PyTorch optimizer.\n",
    "Return a Python float loss value."
   ]
  },
  {
   "cell_type": "code",
   "id": "24c815a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T11:48:34.500250Z",
     "start_time": "2026-02-23T11:48:34.490861Z"
    }
   },
   "source": [
    "def train_step_mse(\n",
    "    model: nn.Module,\n",
    "    batch: tuple[torch.Tensor, torch.Tensor],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    One MSE train step using standard torch optimizer.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    optimizer.zero_grad()\n",
    "    inputs, targets = batch\n",
    "    predictions = model(inputs)\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(predictions, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "eb996eb7",
   "metadata": {},
   "source": [
    "## Parameter initialization\n",
    "\n",
    "Initialization matters because it controls signal and gradient scales at the start of training.\n",
    "\n",
    "### Fan-in / fan-out\n",
    "- `fan_in`: number of input connections to a unit\n",
    "- `fan_out`: number of output connections from a unit\n",
    "\n",
    "For a Linear layer weight of shape `(out_features, in_features)`:\n",
    "- `fan_in = in_features`\n",
    "- `fan_out = out_features`\n",
    "\n",
    "### Common schemes\n",
    "- **Xavier / Glorot** (often good for tanh / linear-ish nets):\n",
    "  keeps variance stable across layers when activations are roughly symmetric.\n",
    "- **Kaiming / He** (often good for ReLU-like nets):\n",
    "  accounts for the fact that ReLU zeroes out about half the inputs.\n",
    "\n",
    "In this section you’ll implement Xavier uniform and Kaiming uniform and use them to initialize `nn.Linear`.\n",
    "We also always zero the bias unless explicitly told otherwise."
   ]
  },
  {
   "cell_type": "code",
   "id": "2c34eff1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T11:59:43.507436Z",
     "start_time": "2026-02-23T11:59:43.499667Z"
    }
   },
   "source": [
    "def fan_in_fan_out(weight: torch.Tensor) -> tuple[int, int]:\n",
    "    \"\"\"Compute (fan_in, fan_out) for a weight tensor.\"\"\"\n",
    "    # TODO: implement\n",
    "    return weight.shape[1], weight.shape[0]"
   ],
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "d421c7bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T12:06:45.816872Z",
     "start_time": "2026-02-23T12:06:45.802744Z"
    }
   },
   "source": [
    "def xavier_uniform_(weight: torch.Tensor, gain: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    In-place Xavier/Glorot uniform init:\n",
    "      bound = gain * sqrt(6 / (fan_in + fan_out))\n",
    "      U(-bound, bound)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    fan_in, fan_out = fan_in_fan_out(weight)\n",
    "    bound = gain * (6 / (fan_in + fan_out))**0.5\n",
    "    return weight.uniform_(-bound, bound)"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "e59d69d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T12:09:08.023397Z",
     "start_time": "2026-02-23T12:09:08.014044Z"
    }
   },
   "source": [
    "def kaiming_uniform_(weight: torch.Tensor, nonlinearity: str = \"relu\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    In-place Kaiming/He uniform init.\n",
    "\n",
    "    Follow this common choice:\n",
    "      gain = sqrt(2) for ReLU\n",
    "      std = gain / sqrt(fan_in)\n",
    "      bound = sqrt(3) * std\n",
    "      U(-bound, bound)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    fan_in, fan_out = fan_in_fan_out(weight)\n",
    "    gain = 2**0.5 if nonlinearity == \"relu\" else 1\n",
    "    std = gain / (fan_in**0.5)\n",
    "    bound = 3**0.5 * std\n",
    "    return weight.uniform_(-bound, bound)"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "af69be99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T12:09:08.169568Z",
     "start_time": "2026-02-23T12:09:08.164161Z"
    }
   },
   "source": [
    "def init_linear_(layer: nn.Linear, scheme: str = \"xavier\") -> nn.Linear:\n",
    "    \"\"\"\n",
    "    Initialize an nn.Linear in-place.\n",
    "\n",
    "    scheme:\n",
    "      - \"xavier\"\n",
    "      - \"kaiming_relu\"\n",
    "      - \"zero\" (weights and bias = 0)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    with torch.no_grad():\n",
    "        if scheme == \"xavier\":\n",
    "            xavier_uniform_(layer.weight)\n",
    "        elif scheme == \"kaiming_relu\":\n",
    "            kaiming_uniform_(layer.weight, nonlinearity=\"relu\")\n",
    "        elif scheme == \"zero\":\n",
    "            layer.bias.zero_()\n",
    "            layer.weight.data.fill_(0)"
   ],
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-23T12:06:46.245285Z",
     "start_time": "2026-02-23T12:06:46.243308Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "b4201c8381031c54",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8a890d885727ef37"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
