{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfac88e5",
   "metadata": {},
   "source": [
    "# Exercise 2: PyTorch core\n",
    "\n",
    "In this exercise you’ll build core PyTorch “muscle memory” that you’ll reuse in basically every model you write:\n",
    "\n",
    "- **Autograd**: how gradients are created, how they accumulate, and how to compute gradients for one or multiple inputs.\n",
    "- **Dataloading**: writing small `Dataset`s, using `DataLoader`, and custom `collate_fn`.\n",
    "- **Optimizers**: implementing **AdamW** updates from scratch (state, bias correction, weight decay).\n",
    "- **Training basics**: a clean single training step.\n",
    "- **Initialization**: fan-in/out and common initializers (Xavier / Kaiming), plus a helper to init `nn.Linear`.\n",
    "\n",
    "As before: fill in all `TODO`s without changing function names or signatures.\n",
    "When debugging, print shapes/dtypes/devices, and write tiny sanity checks (e.g. compare to PyTorch’s built-ins).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0145b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419608fc",
   "metadata": {},
   "source": [
    "## Autograd fundamentals\n",
    "\n",
    "PyTorch builds a computation graph when you apply operations to tensors with `requires_grad=True`.\n",
    "Calling `backward()` (or `torch.autograd.grad`) computes gradients by traversing that graph.\n",
    "\n",
    "### Key concepts\n",
    "- **Leaf tensor**: a tensor created by you (not the result of an operation) with `requires_grad=True`. Leaf tensors can store gradients in `.grad`.\n",
    "- **Gradient accumulation**: calling `backward()` adds into `.grad` (it does not overwrite). You must reset gradients between steps/calls.\n",
    "- **`torch.autograd.grad` vs `.backward()`**\n",
    "  - `torch.autograd.grad(f, x)` returns `df/dx` directly and does not write into `x.grad` unless you explicitly do so.\n",
    "  - `f.backward()` writes gradients into `.grad` of leaf tensors.\n",
    "\n",
    "In the next functions you’ll compute gradients for a simple scalar function such as `f(x) = sum(x^2)` using both APIs.\n",
    "\n",
    "### `torch.no_grad()`\n",
    "Wrap inference-only code to avoid tracking gradients and building graphs:\n",
    "- saves memory\n",
    "- speeds up evaluation\n",
    "\n",
    "### `detach()`\n",
    "`y = x.detach()` returns a tensor that shares data with `x` but is **not connected** to the autograd graph.\n",
    "This is useful when you want to treat something as a constant target.\n",
    "\n",
    "### `model.train()` vs `model.eval()`\n",
    "- `train()` enables training behavior (e.g. dropout active, batchnorm updates running stats).\n",
    "- `eval()` enables inference behavior (e.g. dropout off, batchnorm uses running stats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7724fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_with_autograd_grad(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute gradient of f(x) = sum(x^2) using torch.autograd.grad\n",
    "\n",
    "    Requirements:\n",
    "    - Do not call .backward().\n",
    "    - x should require grad inside the function (don't assume it does).\n",
    "    - Must return df/dx\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0d32cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grad_with_backward(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute gradient of f(x) = sum(x^2) using .backward().\n",
    "\n",
    "    Requirements:\n",
    "    - Must return df/dx\n",
    "    - Must not leak gradients across calls (watch x.grad accumulation)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec40995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_wrt_multiple_inputs(\n",
    "    a: torch.Tensor, b: torch.Tensor,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute gradients w.r.t. multiple inputs. The function is f(a, b) = sum(a^2 + ab).\n",
    "\n",
    "    Return:\n",
    "        (df/da, df/db)\n",
    "\n",
    "    Requirements:\n",
    "    - Use torch.autograd.grad\n",
    "    - Ensure both a and b require grad in this function.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a026b",
   "metadata": {},
   "source": [
    "## Dataloading\n",
    "\n",
    "In PyTorch, a `Dataset` defines how to fetch a *single* training example, and a `DataLoader` handles:\n",
    "- batching\n",
    "- shuffling\n",
    "- parallel workers\n",
    "- optional custom batching logic via `collate_fn`\n",
    "\n",
    "### `Dataset` in one sentence\n",
    "A `Dataset` only needs:\n",
    "- `__len__`: number of items\n",
    "- `__getitem__`: return one item (e.g. `(x, y)`)\n",
    "\n",
    "### Why `collate_fn` matters\n",
    "The default DataLoader collation stacks items along a new batch dimension.\n",
    "That works for fixed-size tensors, but it breaks for **variable-length sequences**.\n",
    "\n",
    "So we’ll implement padding ourselves:\n",
    "- Convert a list of 1D token sequences into a padded tensor `(B, T_max)`\n",
    "- Track `lengths` and a `padding_mask`\n",
    "\n",
    "### Mask convention for padding\n",
    "For padding masks in this exercise:\n",
    "- `padding_mask[b, t] == True` means **this is padding / invalid**\n",
    "- `padding_mask[b, t] == False` means **this is a real token**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbd7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab0d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorPairDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Minimal dataset wrapping (x, y).\n",
    "\n",
    "    x: (N, ...)\n",
    "    y: (N, ...)\n",
    "\n",
    "    N is the number of samples. The dataset should return tuples of (x[i], y[i]).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, x: torch.Tensor, y: torch.Tensor):\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e02fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextTokenDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Next-token prediction dataset.\n",
    "\n",
    "    Given tokens of shape (N, T), produce:\n",
    "      input_ids  = tokens[:, :-1]\n",
    "      target_ids = tokens[:, 1:]\n",
    "\n",
    "    Return per item:\n",
    "      (input_ids, target_ids)\n",
    "\n",
    "    Notes:\n",
    "    - Returned tensors should be 1D of length (T-1).\n",
    "    - dtype should remain integer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens: torch.Tensor):\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5d3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandomCropSequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Sequence dataset that returns random crops of fixed length.\n",
    "\n",
    "    tokens: (N, T_total)\n",
    "    crop_len: L\n",
    "\n",
    "    For each __getitem__:\n",
    "      - sample a start index s so that s+L <= T_total\n",
    "      - return tokens[idx, s:s+L]\n",
    "\n",
    "    Requirements:\n",
    "    - Use a torch.Generator for deterministic behavior if seed is provided.\n",
    "    - Do NOT use Python's random module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens: torch.Tensor, crop_len: int, seed: int | None = None):\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        # TODO: implement\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73593f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PaddedBatch:\n",
    "    \"\"\"\n",
    "    A padded batch for variable-length sequences.\n",
    "\n",
    "    tokens: LongTensor (B, T_max)\n",
    "    lengths: LongTensor (B,)\n",
    "    padding_mask: BoolTensor (B, T_max) where True means \"this is padding\"\n",
    "    \"\"\"\n",
    "\n",
    "    tokens: torch.Tensor\n",
    "    lengths: torch.Tensor\n",
    "    padding_mask: torch.Tensor\n",
    "\n",
    "\n",
    "def pad_1d_sequences(seqs: list[torch.Tensor], pad_value: int = 0) -> PaddedBatch:\n",
    "    \"\"\"\n",
    "    Pad a list of 1D integer tensors to the same length.\n",
    "\n",
    "    Requirements:\n",
    "    - Return PaddedBatch(tokens, lengths, padding_mask)\n",
    "    - padding_mask[b, t] == True iff t >= lengths[b]\n",
    "    - tokens should be dtype long, if not cast them\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2df849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_next_token_batch(\n",
    "    batch: list[tuple[torch.Tensor, torch.Tensor]], pad_value: int = 0\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Collate for NextTokenDataset samples that may have variable lengths.\n",
    "\n",
    "    batch: list of (input_ids, target_ids), each 1D\n",
    "\n",
    "    Return dict with:\n",
    "      - input_ids: (B, T_max)\n",
    "      - target_ids: (B, T_max)\n",
    "      - attention_mask: (B, T_max) where True means \"keep\" (NOT padding)\n",
    "      - padding_mask: (B, T_max) where True means \"padding\"\n",
    "\n",
    "    Requirements:\n",
    "    - pad input_ids and target_ids consistently\n",
    "    - attention_mask is the logical NOT of padding_mask\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904bdd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(\n",
    "    dataset: Dataset,\n",
    "    batch_size: int,\n",
    "    shuffle: bool = True,\n",
    "    drop_last: bool = False,\n",
    "    collate_fn=None,\n",
    "    num_workers: int = 0,\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Create a DataLoader with optional collate_fn.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab920b",
   "metadata": {},
   "source": [
    "## Optimizers (AdamW from scratch)\n",
    "\n",
    "PyTorch optimizers keep **state** for each parameter (e.g. moment estimates in Adam).\n",
    "In this section you’ll implement **AdamW**, which is Adam + *decoupled* weight decay.\n",
    "\n",
    "### AdamW state\n",
    "For each parameter tensor `p` we store:\n",
    "- `m`: first moment (EMA of gradients)\n",
    "- `v`: second moment (EMA of squared gradients)\n",
    "- `t`: step counter\n",
    "\n",
    "### Update overview (high level)\n",
    "1) Update moments `m, v`\n",
    "2) Bias-correct them (`m_hat, v_hat`)\n",
    "3) Apply parameter update:\n",
    "   `p -= lr * ( m_hat / (sqrt(v_hat) + eps) + weight_decay * p )`\n",
    "\n",
    "Notes:\n",
    "- This update is **in-place** (mutates `p`).\n",
    "- Gradients should not be modified.\n",
    "- State tensors must match parameter shape/device/dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4c124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AdamWState:\n",
    "    \"\"\"\n",
    "    Per-parameter AdamW state.\n",
    "\n",
    "    m: first moment\n",
    "    v: second moment\n",
    "    t: step count\n",
    "    \"\"\"\n",
    "\n",
    "    m: torch.Tensor\n",
    "    v: torch.Tensor\n",
    "    t: int\n",
    "\n",
    "\n",
    "def init_adamw_state(p: torch.Tensor) -> AdamWState:\n",
    "    \"\"\"\n",
    "    Initialize AdamW state tensors for a parameter tensor p.\n",
    "\n",
    "    What to create:\n",
    "    - m: zeros like p, same shape/device/dtype\n",
    "    - v: zeros like p, same shape/device/dtype\n",
    "    - t: step counter starting at 0\n",
    "\n",
    "    Notes / requirements:\n",
    "    - Use torch.zeros_like(p) for m and v.\n",
    "    - Do NOT attach gradients to the state (initialize under torch.no_grad()).\n",
    "    - t starts at 0. In adamw_step_, increment t to 1 on the first update *before*\n",
    "      computing bias correction terms (1 - beta1^t) and (1 - beta2^t).\n",
    "    - State tensors must live on the same device as p (CPU vs GPU) and have the\n",
    "      same dtype as p.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eeaa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamw_step_(\n",
    "    p: torch.Tensor,\n",
    "    grad: torch.Tensor,\n",
    "    state: AdamWState,\n",
    "    lr: float,\n",
    "    betas: tuple[float, float] = (0.9, 0.999),\n",
    "    eps: float = 1e-8,\n",
    "    weight_decay: float = 0.01,\n",
    ") -> AdamWState:\n",
    "    \"\"\"\n",
    "    In-place AdamW parameter update (updates p).\n",
    "\n",
    "    Algorithm (AdamW):\n",
    "      m = beta1*m + (1-beta1)*grad\n",
    "      v = beta2*v + (1-beta2)*grad^2\n",
    "      m_hat = m / (1 - beta1^t)\n",
    "      v_hat = v / (1 - beta2^t)\n",
    "      p = p - lr * (m_hat / (sqrt(v_hat) + eps) + weight_decay * p)\n",
    "\n",
    "    Requirements:\n",
    "    - Update p in-place.\n",
    "    - Return updated state (with incremented t).\n",
    "    - Do not modify grad.\n",
    "    - Should work for any tensor shape.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adamw_step_many_(\n",
    "    params: list[torch.Tensor],\n",
    "    grads: list[torch.Tensor],\n",
    "    states: list[AdamWState],\n",
    "    lr: float,\n",
    "    betas: tuple[float, float] = (0.9, 0.999),\n",
    "    eps: float = 1e-8,\n",
    "    weight_decay: float = 0.01,\n",
    ") -> list[AdamWState]:\n",
    "    \"\"\"\n",
    "    Apply AdamW to many parameters.\n",
    "\n",
    "    Requirements:\n",
    "    - len(params) == len(grads) == len(states)\n",
    "    - Update each param in-place.\n",
    "    - Return the list of updated states.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b571bd",
   "metadata": {},
   "source": [
    "## Training basics\n",
    "\n",
    "A minimal training step follows the same pattern almost everywhere:\n",
    "\n",
    "1) set model to train mode\n",
    "2) reset gradients\n",
    "3) forward pass\n",
    "4) compute loss\n",
    "5) backward pass\n",
    "6) step optimizer\n",
    "\n",
    "In this exercise you’ll implement a single MSE training step using a standard PyTorch optimizer.\n",
    "Return a Python float loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c815a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_mse(\n",
    "    model: nn.Module,\n",
    "    batch: tuple[torch.Tensor, torch.Tensor],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    One MSE train step using standard torch optimizer.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb996eb7",
   "metadata": {},
   "source": [
    "## Parameter initialization\n",
    "\n",
    "Initialization matters because it controls signal and gradient scales at the start of training.\n",
    "\n",
    "### Fan-in / fan-out\n",
    "- `fan_in`: number of input connections to a unit\n",
    "- `fan_out`: number of output connections from a unit\n",
    "\n",
    "For a Linear layer weight of shape `(out_features, in_features)`:\n",
    "- `fan_in = in_features`\n",
    "- `fan_out = out_features`\n",
    "\n",
    "### Common schemes\n",
    "- **Xavier / Glorot** (often good for tanh / linear-ish nets):\n",
    "  keeps variance stable across layers when activations are roughly symmetric.\n",
    "- **Kaiming / He** (often good for ReLU-like nets):\n",
    "  accounts for the fact that ReLU zeroes out about half the inputs.\n",
    "\n",
    "In this section you’ll implement Xavier uniform and Kaiming uniform and use them to initialize `nn.Linear`.\n",
    "We also always zero the bias unless explicitly told otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c34eff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fan_in_fan_out(weight: torch.Tensor) -> tuple[int, int]:\n",
    "    \"\"\"Compute (fan_in, fan_out) for a weight tensor.\"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d421c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_uniform_(weight: torch.Tensor, gain: float = 1.0) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    In-place Xavier/Glorot uniform init:\n",
    "      bound = gain * sqrt(6 / (fan_in + fan_out))\n",
    "      U(-bound, bound)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59d69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_uniform_(weight: torch.Tensor, nonlinearity: str = \"relu\") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    In-place Kaiming/He uniform init.\n",
    "\n",
    "    Follow this common choice:\n",
    "      gain = sqrt(2) for ReLU\n",
    "      std = gain / sqrt(fan_in)\n",
    "      bound = sqrt(3) * std\n",
    "      U(-bound, bound)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af69be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_linear_(layer: nn.Linear, scheme: str = \"xavier\") -> nn.Linear:\n",
    "    \"\"\"\n",
    "    Initialize an nn.Linear in-place.\n",
    "\n",
    "    scheme:\n",
    "      - \"xavier\"\n",
    "      - \"kaiming_relu\"\n",
    "      - \"zero\" (weights and bias = 0)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
