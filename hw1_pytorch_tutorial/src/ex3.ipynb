{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b9f180e",
   "metadata": {},
   "source": [
    "# Exercise 3: Neural networks in PyTorch\n",
    "\n",
    "In this exercise you’ll implement small neural-network building blocks from scratch and use them to train a simple classifier.\n",
    "\n",
    "You’ll cover:\n",
    "- **Basic layers**: Linear, Embedding, Dropout\n",
    "- **Normalization**: LayerNorm and RMSNorm\n",
    "- **MLPs + residual**: composing layers into deeper networks\n",
    "- **Classification**: generating a learnable dataset, implementing cross-entropy from logits, and writing a minimal training loop\n",
    "\n",
    "As before: fill in all `TODO`s without changing function names or signatures.\n",
    "Use small sanity checks and compare to PyTorch reference implementations when useful."
   ]
  },
  {
   "cell_type": "code",
   "id": "948aeb0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:32.214149Z",
     "start_time": "2026-02-27T14:20:29.356211Z"
    }
   },
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "93aaebe3",
   "metadata": {},
   "source": [
    "## Basic layers\n",
    "\n",
    "In this section you’ll implement a few core layers that appear everywhere:\n",
    "\n",
    "### `Linear`\n",
    "A fully-connected layer that follows nn.Linear conventions:  \n",
    "`y = x @ Wᵀ + b`\n",
    "\n",
    "Important details:\n",
    "- Parameters should be registered as `nn.Parameter`\n",
    "- Store weight as (out_features, in_features) like nn.Linear.\n",
    "- The forward pass should support leading batch dimensions: `x` can be shape `(..., in_features)`\n",
    "\n",
    "### `Embedding`\n",
    "An embedding table maps integer ids to vectors:\n",
    "- input: token ids `idx` of shape `(...,)`\n",
    "- output: vectors of shape `(..., embedding_dim)`\n",
    "\n",
    "This is essentially a learnable lookup table.\n",
    "\n",
    "### `Dropout`\n",
    "Dropout randomly zeroes activations during training to reduce overfitting.\n",
    "Implementation details:\n",
    "- Only active in `model.train()` mode\n",
    "- In training: drop with probability `p` and scale the kept values by `1/(1-p)` so the expected value stays the same\n",
    "- In eval: return the input unchanged\n",
    "\n",
    "## Instructions\n",
    "- Do not use PyTorch reference modules for the parts you implement (e.g. don’t call nn.Linear inside your Linear).\n",
    "- You may use standard tensor ops that you learned before (matmul, sum, mean, rsqrt, indexing, etc.).\n",
    "- Use a parameter initialization method of your choice. We recommend something like Xavier-uniform.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fa9b6f20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:34.109087Z",
     "start_time": "2026-02-27T14:20:34.043128Z"
    }
   },
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        # TODO: initialize parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fan_in, fan_out = self.weight.shape[1], self.weight.shape[0]\n",
    "            bound = (6 / (fan_in + fan_out))**0.5\n",
    "            self.weight.uniform_(-bound, bound)\n",
    "            if self.bias is not None:\n",
    "                self.bias.zero_()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (..., in_features)\n",
    "        return: (..., out_features)\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        out = torch.matmul(x, self.weight.T)\n",
    "        out += self.bias if self.bias is not None else 0\n",
    "        return out\n",
    "    \n",
    "test_layer = Linear(5, 2)\n",
    "x = torch.ones(2,5)\n",
    "y = test_layer(x)\n",
    "print(y)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1930,  0.3951],\n",
      "        [-2.1930,  0.3951]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b2241e12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:34.282509Z",
     "start_time": "2026-02-27T14:20:34.273675Z"
    }
   },
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        # TODO: initialize\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.weight = nn.Parameter(torch.empty(num_embeddings, embedding_dim))\n",
    "        with torch.no_grad():\n",
    "            fan_in, fan_out = self.num_embeddings, self.embedding_dim\n",
    "            bound = (6 / (fan_in + fan_out))**0.5\n",
    "            self.weight.uniform_(-bound, bound)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        idx: (...,) int64\n",
    "        return: (..., embedding_dim)\n",
    "        \"\"\"\n",
    "        # TODO: implement (index into weight)\n",
    "        return self.weight[idx]\n",
    "\n",
    "test_embedding = Embedding(5, 2)\n",
    "tok = 4\n",
    "print(test_embedding(tok))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2519, -0.3782], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "3390686f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:35.344833Z",
     "start_time": "2026-02-27T14:20:35.340948Z"
    }
   },
   "source": [
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p: float):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        In train mode: drop with prob p and scale by 1/(1-p).\n",
    "        In eval mode: return x unchanged.\n",
    "        \"\"\"\n",
    "        # TODO: implement without using nn.Dropout\n",
    "        if not self.training or self.p == 0:\n",
    "            return x\n",
    "        \n",
    "        mask = (torch.rand_like(x) > self.p).float() # sample values from 0 and 1 and mask out if smaller than p\n",
    "        return x * mask / (1.0-self.p)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "eef77371",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Normalization layers help stabilize training by controlling activation statistics.\n",
    "\n",
    "### LayerNorm\n",
    "LayerNorm normalizes each example across its **feature dimension** (the last dimension):\n",
    "\n",
    "- compute mean and variance over the last dimension\n",
    "- normalize: `(x - mean) / sqrt(var + eps)`\n",
    "- apply learnable per-feature scale and shift (`weight`, `bias`)\n",
    "\n",
    "**In this exercise, assume `elementwise_affine=True` (always include `weight` and `bias`).**  \n",
    "`weight` and `bias` each have shape `(D,)`.\n",
    "\n",
    "LayerNorm is widely used in transformers because it does not depend on batch statistics.\n",
    "\n",
    "### RMSNorm\n",
    "RMSNorm is similar to LayerNorm but normalizes using only the root-mean-square:\n",
    "- `x / sqrt(mean(x^2) + eps)` over the last dimension\n",
    "- usually includes a learnable scale (`weight`)\n",
    "- no mean subtraction\n",
    "\n",
    "RMSNorm is popular in modern LLMs because it's faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "aeaceef2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:36.461171Z",
     "start_time": "2026-02-27T14:20:36.454940Z"
    }
   },
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(\n",
    "        self, normalized_shape: int, eps: float = 1e-5, elementwise_affine: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # TODO: implement\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        if elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "            self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize over the last dimension.\n",
    "        x: (..., D)\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        centered_data = x - torch.mean(x, dim=-1, keepdim=True)\n",
    "        scale = torch.sqrt(torch.var(centered_data, dim=-1, keepdim=True, unbiased=False) + self.eps)\n",
    "        if self.elementwise_affine:\n",
    "            out = centered_data / scale * self.weight + self.bias\n",
    "        else:\n",
    "            out = centered_data / scale\n",
    "        \n",
    "        return out\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "08a2ac46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:36.766858Z",
     "start_time": "2026-02-27T14:20:36.761656Z"
    }
   },
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape: int, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        # TODO: implement\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        RMSNorm: x / sqrt(mean(x^2) + eps) * weight\n",
    "        over the last dimension.\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        out = x / rms * self.weight\n",
    "        return out"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "53f2b352",
   "metadata": {},
   "source": [
    "## MLPs and residual networks\n",
    "\n",
    "Now you’ll build larger networks by composing layers.\n",
    "\n",
    "### MLP\n",
    "An MLP is a stack of `depth` Linear layers with non-linear activations (use GELU) in between.\n",
    "In this exercise you’ll support:\n",
    "- configurable depth\n",
    "- a hidden dimension\n",
    "- optional LayerNorm between layers (a common stabilization trick)\n",
    "\n",
    "A key skill is building networks using `nn.ModuleList` / `nn.Sequential` while keeping shapes consistent.\n",
    "\n",
    "### Transformer-style FeedForward (FFN)\n",
    "A transformer block contains a position-wise feedforward network:\n",
    "- `D -> 4D -> D` (by default)\n",
    "- activation is typically **GELU**\n",
    "\n",
    "This is essentially an MLP applied independently at each token position.\n",
    "\n",
    "### Residual wrapper\n",
    "Residual connections are the simplest form of “skip connection”:\n",
    "- output is `x + fn(x)`\n",
    "\n",
    "They improve gradient flow and allow training deeper networks more reliably."
   ]
  },
  {
   "cell_type": "code",
   "id": "22436c9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:36.993238Z",
     "start_time": "2026-02-27T14:20:36.986711Z"
    }
   },
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        hidden_dim: int,\n",
    "        out_dim: int,\n",
    "        depth: int,\n",
    "        use_layernorm: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # TODO: build modules (list of Linear + activation)\n",
    "        # Optionally insert LayerNorm between layers.\n",
    "        self.use_layernorm = use_layernorm\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(Linear(in_dim, hidden_dim))\n",
    "        if use_layernorm:\n",
    "            self.layers.append(LayerNorm(hidden_dim))\n",
    "        \n",
    "        for _ in range(depth - 1):\n",
    "            self.layers.append(Linear(hidden_dim, hidden_dim))\n",
    "            if use_layernorm:\n",
    "                self.layers.append(LayerNorm(hidden_dim))\n",
    "        \n",
    "        self.output_layer = Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: implement\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Linear):\n",
    "                x = layer(x)\n",
    "                x = torch.nn.functional.gelu(x)\n",
    "            elif isinstance(layer, LayerNorm):\n",
    "                x = layer(x)\n",
    "            \n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "d2169774",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:37.255625Z",
     "start_time": "2026-02-27T14:20:37.248537Z"
    }
   },
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-style FFN: D -> 4D -> D (default)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int | None = None):\n",
    "        super().__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        # TODO: create two Linear layers and choose an activation (GELU)\n",
    "        self.fc = MLP(d_model, d_ff, d_model, depth=2, use_layernorm=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: implement\n",
    "        return self.fc(x)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "80eef3a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:37.385816Z",
     "start_time": "2026-02-27T14:20:37.381232Z"
    }
   },
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn: nn.Module):\n",
    "        super().__init__()\n",
    "        # TODO: implement\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        # TODO: return x + fn(x, ...)\n",
    "        return x + self.fn(x, *args, **kwargs)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "b8b19d1a",
   "metadata": {},
   "source": [
    "## Classification problem\n",
    "\n",
    "In this section you’ll put everything together in a minimal MNIST classification experiment.\n",
    "\n",
    "You will:\n",
    "1) download and load the MNIST dataset\n",
    "2) implement cross-entropy from logits (stable, using log-softmax)\n",
    "3) build a simple MLP-based classifier (flatten MNIST images first)\n",
    "4) write a minimal training loop\n",
    "5) report train loss curve and final accuracy\n",
    "\n",
    "The goal here is not to reach state-of-the-art accuracy, but to understand the full pipeline:\n",
    "data → model → logits → loss → gradients → parameter update.\n",
    "\n",
    "### Model notes\n",
    "- We want you to combine the MLP we implemented above with the classification head we define below into one model \n",
    "\n",
    "### MNIST notes\n",
    "- MNIST images are `28×28` grayscale.\n",
    "- After `ToTensor()`, each image has shape `(1, 28, 28)` and values in `[0, 1]`.\n",
    "- For an MLP classifier, we flatten to a vector of length `784`.\n",
    "\n",
    "## Deliverables\n",
    "- Include a plot of your train loss curve in the video submission as well as a final accuracy. \n",
    "- **NOTE** Here we don't grade on model performance but we expect you to achieve at least 70% accuracy to confirm a correct model implementation."
   ]
  },
  {
   "cell_type": "code",
   "id": "00c05009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:42.107864Z",
     "start_time": "2026-02-27T14:20:37.779517Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "6306a4e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:42.581427Z",
     "start_time": "2026-02-27T14:20:42.110850Z"
    }
   },
   "source": [
    "transform = transforms.ToTensor()  # -> float32 in [0,1], shape (1, 28, 28)\n",
    "\n",
    "train_ds = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "test_ds  = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "\n",
    "# TODO: define the dataloaders"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "a3781450",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:42.594792Z",
     "start_time": "2026-02-27T14:20:42.587216Z"
    }
   },
   "source": [
    "def cross_entropy_from_logits(\n",
    "    logits: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute mean cross-entropy loss from logits.\n",
    "\n",
    "    logits: (B, C)\n",
    "    targets: (B,) int64\n",
    "\n",
    "    Requirements:\n",
    "    - Use log-softmax for stability (do not use torch.nn.CrossEntropyLoss, we check this in the autograder).\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    target_log_probs = log_probs.gather(dim=1, index=targets.unsqueeze(1))\n",
    "    loss = -target_log_probs.mean()\n",
    "    return loss"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "71242e3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:42.607032Z",
     "start_time": "2026-02-27T14:20:42.601639Z"
    }
   },
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, d_in: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        # TODO: implement\n",
    "        self.fc = MLP(d_in, d_in, num_classes, depth=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (..., d_in)\n",
    "        return: (..., num_classes) logits\n",
    "        \"\"\"\n",
    "        # TODO: implement\n",
    "        return self.fc(x)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "fa3bd0e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:42.627165Z",
     "start_time": "2026-02-27T14:20:42.610479Z"
    }
   },
   "source": [
    "@torch.no_grad()\n",
    "def accuracy(model, loader, device=\"cpu\"):\n",
    "    # TODO: You can use this function to evaluate your model accuracy.\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x,y in loader:\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        \n",
    "        logits = model(x)\n",
    "        \n",
    "        if logits.ndim == 3:\n",
    "            logits = logits[:, -1, :]\n",
    "        \n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        correct += (preds == y).sum().item()\n",
    "        total += y.size(0)\n",
    "        \n",
    "    return correct / total"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "20555e4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:20:42.650786Z",
     "start_time": "2026-02-27T14:20:42.631303Z"
    }
   },
   "source": [
    "def train_classifier(\n",
    "    model: nn.Module,\n",
    "    train_data_loader: DataLoader,\n",
    "    test_data_loader: DataLoader,\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    "    seed: int = 0,\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    Minimal training loop for MNIST classification.\n",
    "\n",
    "    Steps:\n",
    "    - define optimizer\n",
    "    - for each epoch:\n",
    "        - sample minibatches\n",
    "        - forward -> cross-entropy -> backward -> optimizer step\n",
    "      - compute test accuracy at the end of each epoch\n",
    "    - return list of training losses (one per update step)\n",
    "\n",
    "    Requirements:\n",
    "    - call model.train() during training and model.eval() during evaluation\n",
    "    - do not use torch.nn.CrossEntropyLoss (use your cross_entropy_from_logits)\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    accuracies = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "        for x,y in train_data_loader:\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits = model(x)\n",
    "            \n",
    "            loss = cross_entropy_from_logits(logits, y)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "        avg_loss /= len(train_data_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        test_acc = accuracy(model, test_data_loader, device)\n",
    "        accuracies.append(test_acc)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Test Acc: {test_acc:.4f}\")\n",
    "    \n",
    "    return train_losses\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "a7cdad3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-27T14:22:19.133020Z",
     "start_time": "2026-02-27T14:20:42.656951Z"
    }
   },
   "source": [
    "model = ClassificationHead(784, 10)\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=True)\n",
    "train_losses = train_classifier(model, train_loader, test_loader, lr=1e-3, epochs=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Loss: 0.2220 - Test Acc: 0.9698\n",
      "Epoch 2/5 - Loss: 0.0829 - Test Acc: 0.9780\n",
      "Epoch 3/5 - Loss: 0.0493 - Test Acc: 0.9780\n",
      "Epoch 4/5 - Loss: 0.0340 - Test Acc: 0.9803\n",
      "Epoch 5/5 - Loss: 0.0211 - Test Acc: 0.9818\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
