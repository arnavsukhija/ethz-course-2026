{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7c97e4a6",
      "metadata": {
        "id": "7c97e4a6"
      },
      "source": [
        "# Exercise 4: Transformers on Images + GLU-MLP Ablations (ViT × GLU Variants)\n",
        "\n",
        "## In this exercise you will combine two influential ideas:\n",
        "\n",
        "Vision Transformers (ViT) from “An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale” (Dosovitskiy et al., 2020) https://arxiv.org/pdf/2010.11929:\n",
        "ViT shows that you can treat an image like a sequence of tokens by splitting it into non-overlapping patches (e.g. 16×16 in the paper), embedding each patch into a vector, adding positional information, and then applying standard Transformer blocks for classification.\n",
        "\n",
        "Gated MLPs (GLU variants) from “GLU Variants Improve Transformer” (Shazeer, 2020) https://arxiv.org/pdf/2002.05202:\n",
        "Shazeer proposes replacing the standard Transformer feed-forward layer (FFN/MLP) with gated linear unit (GLU) variants such as GEGLU and SwiGLU, which often improves training dynamics and final performance under comparable compute/parameter budgets.\n",
        "\n",
        "## What you will do\n",
        "\n",
        "You will implement a tiny ViT-style classifier for MNIST, then run a controlled ablation where you replace the MLP inside each Transformer block:\n",
        "\n",
        "Baseline FFN (GELU):\n",
        "Linear(d_model → d_ff) → GELU → Linear(d_ff → d_model)\n",
        "\n",
        "GLU-family MLPs (choose at least two and justify):\n",
        "\n",
        "GEGLU, SwiGLU, other activation functions\n",
        "\n",
        "Your goal is to evaluate whether these GLU variants change:\n",
        "\n",
        "- convergence speed (loss vs steps),\n",
        "\n",
        "- final test accuracy,\n",
        "\n",
        "- and/or stability across runs.\n",
        "\n",
        "## Key ViT concepts you will implement\n",
        "\n",
        "- To convert MNIST images into Transformer tokens, you will:\n",
        "  Patchify each 28×28 image into non-overlapping P×P patches.\n",
        "  If P=4, then you get a 7×7 patch grid → 49 tokens per image.\n",
        "\n",
        "- Embed patches with a linear layer: patch vectors → d_model.\n",
        "\n",
        "- Add positional embeddings so the model knows where each patch came from.\n",
        "\n",
        "- Apply n_layers Transformer encoder blocks.\n",
        "\n",
        "- Pool token features (e.g., mean pooling) and project to 10 classes.\n",
        "\n",
        "## Key GLU concept you will implement\n",
        "\n",
        "GLU-style MLPs replace a standard FFN with a gating mechanism:\n",
        "compute two projections a and b, apply a nonlinearity to a (variant-dependent), multiply elementwise: act(a) * b, project back to d_model.\n",
        "To keep the comparison fair, use the 2/3 width rule from Shazeer.\n",
        "\n",
        "What we provide vs what you implement\n",
        "\n",
        "### We provide:\n",
        "\n",
        "- MNIST loading + dataloaders\n",
        "\n",
        "- a minimal training loop structure (AdamW)\n",
        "\n",
        "- a suggested small model configuration that runs on CPU\n",
        "\n",
        "### You implement:\n",
        "\n",
        "- patch tokenization (patchify)\n",
        "\n",
        "- patch embedding + positional embedding strategy\n",
        "\n",
        "- a pre-LN Transformer encoder block using nn.MultiheadAttention\n",
        "\n",
        "- at least two GLU MLP variants + one FFN baseline\n",
        "\n",
        "- metric logging sufficient to support your conclusion\n",
        "\n",
        "## Deliverables\n",
        "\n",
        "Run at least 3 variants (baseline + the activation functions you choose for GLU) and report:\n",
        "\n",
        "- final and best test accuracy\n",
        "\n",
        "- number of trainable parameters\n",
        "\n",
        "- a plot or printed summary of loss/accuracy over epochs\n",
        "\n",
        "- a short discussion of your results"
      ]
    },
    {
      "cell_type": "code",
      "id": "f7aa25d6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-26T11:57:18.255015Z",
          "start_time": "2026-02-26T11:56:57.710461Z"
        },
        "id": "f7aa25d6"
      },
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "id": "c1412943",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-26T12:08:06.940725Z",
          "start_time": "2026-02-26T12:08:06.934397Z"
        },
        "id": "c1412943"
      },
      "source": [
        "def patchify(x: torch.Tensor, patch_size: int) -> torch.Tensor:\n",
        "    \"\"\"Convert images to patch tokens.\"\"\"\n",
        "    # TODO: Implement a tokenization strategy\n",
        "    B,C,H,W = x.shape\n",
        "    G1 = W // patch_size\n",
        "    G2 = H // patch_size\n",
        "    x = x.view(B, C, G1, patch_size, G2, patch_size)\n",
        "    x = x.permute(0, 2, 4, 3, 5, 1)\n",
        "    x =  x.flatten(1, 2)\n",
        "    x = x.flatten(2)\n",
        "    return x"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "id": "166f2d4a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-26T15:25:36.433198Z",
          "start_time": "2026-02-26T15:25:36.332548Z"
        },
        "id": "166f2d4a"
      },
      "source": [
        "# TODO: Add positional encoding as done in the ViT paper and patch projection\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, patch_dim: int, d_model: int):\n",
        "        super().__init__()\n",
        "        # TODO: implement\n",
        "        self.weight = nn.Parameter(torch.empty(d_model, patch_dim))\n",
        "        self.bias = nn.Parameter(torch.empty(d_model))\n",
        "        with torch.no_grad():\n",
        "            fan_in, fan_out = d_model, patch_dim\n",
        "            bound = (6 / (fan_in + fan_out))**0.5\n",
        "            self.weight.uniform_(-bound, bound)\n",
        "            self.bias.zero_()\n",
        "\n",
        "    def forward(self, x_patches: torch.Tensor) -> torch.Tensor:\n",
        "        # TODO: implement\n",
        "        return torch.matmul(x_patches, self.weight.T) + self.bias\n",
        "\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, num_tokens: int, d_model: int):\n",
        "        super().__init__()\n",
        "        # TODO: implement\n",
        "        self.weight = nn.Parameter(torch.empty(1, num_tokens, d_model))\n",
        "        with torch.no_grad():\n",
        "            bound = (6 / (d_model + d_model)**0.5)\n",
        "            self.weight.uniform_(-bound, bound)\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # TODO: implement\n",
        "        return x + self.weight"
      ],
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "id": "8bc753eb",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-26T15:56:34.748051Z",
          "start_time": "2026-02-26T15:56:34.698012Z"
        },
        "id": "8bc753eb"
      },
      "source": [
        "# TODO: Define the variants you want to compare against each other from the GLU paper. Justify your choice.\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Standard Transformer FFN:\n",
        "      x -> Linear(d_model->d_ff) -> GELU -> Dropout -> Linear(d_ff->d_model) -> Dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float):\n",
        "        super().__init__()\n",
        "        # TODO: implement\n",
        "        self.net = nn.Sequential(nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout), nn.Linear(d_ff, d_model), nn.Dropout(dropout))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # TODO: implement\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class GLUFeedForward(nn.Module):\n",
        "    \"\"\"GLU-family FFN\"\"\"\n",
        "    def __init__(self, d_model: int, d_ff_gated: int, dropout: float, variant: str):\n",
        "        super().__init__()\n",
        "        # TODO: implement\n",
        "        self.d_ff_glu = int(d_ff_gated * 2 / 3) #using 2/3 rule\n",
        "\n",
        "        self.w1 = nn.Linear(d_model, self.d_ff_glu*2)\n",
        "        self.w2 = nn.Linear(self.d_ff_glu, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.variant = variant\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # TODO: implement\n",
        "        combined_projection = self.w1(x)\n",
        "\n",
        "        gate, content = torch.chunk(combined_projection, 2, dim=-1)\n",
        "\n",
        "        if self.variant == \"GEGLU\":\n",
        "            gate = F.gelu(gate)\n",
        "        elif self.variant == \"SwiGLU\":\n",
        "            gate = F.silu(gate)\n",
        "        elif self.variant == \"ReGLU\":\n",
        "          gate = F.relu(gate)\n",
        "\n",
        "        x = gate * content # element-wise multiplication\n",
        "\n",
        "        return self.dropout(self.w2(x))"
      ],
      "outputs": [],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "id": "c84d9a34",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-26T16:34:37.894787Z",
          "start_time": "2026-02-26T16:34:37.889547Z"
        },
        "id": "c84d9a34"
      },
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Pre-LN encoder block:\n",
        "      x = x + Dropout(SelfAttn(LN(x)))\n",
        "      x = x + Dropout(MLP(LN(x)))\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, n_heads: int, mlp: nn.Module, dropout: float):\n",
        "        super().__init__()\n",
        "        # TODO: implement. For attention use nn.MultiHeadAttention\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.mlp = mlp\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # TODO: implement\n",
        "        norm_x = self.ln1(x)\n",
        "        attn_out, _ = self.self_attn(norm_x, norm_x, norm_x)\n",
        "        x = x + self.dropout(attn_out)\n",
        "        x = x + self.dropout(self.mlp(self.ln2(x)))\n",
        "        return x"
      ],
      "outputs": [],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "id": "f4388f8e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-26T16:37:06.921433Z",
          "start_time": "2026-02-26T16:37:06.915387Z"
        },
        "id": "f4388f8e"
      },
      "source": [
        "class TinyViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Tiny ViT-style classifier for MNIST.\n",
        "    - patchify -> patch embed -> pos embed -> blocks -> mean pool -> head\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        patch_size: int,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        n_layers: int,\n",
        "        d_ff: int,\n",
        "        dropout: float,\n",
        "        mlp_kind: str,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert 28 % patch_size == 0\n",
        "        grid = 28 // patch_size\n",
        "        self.num_tokens = grid * grid\n",
        "        self.patch_size = patch_size\n",
        "        patch_dim = patch_size * patch_size\n",
        "\n",
        "        # TODO: implement a strategy for embedding the patches\n",
        "        self.patch_embed = PatchEmbed(patch_dim, d_model)\n",
        "        self.pos_embed = PositionalEmbedding(self.num_tokens, d_model)\n",
        "\n",
        "        # TODO: implement a strategy to select the right mlp version for your experiment\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(\n",
        "                d_model=d_model,\n",
        "                n_heads=n_heads,\n",
        "                mlp=(FeedForward(d_model, d_ff, dropout) if mlp_kind == \"baseline\" else GLUFeedForward(d_model, d_ff, dropout, mlp_kind)), # TODO: Feed your mlp to the encoder blocks\n",
        "                dropout=dropout,\n",
        "            )\n",
        "            for _ in range(n_layers)\n",
        "        ])\n",
        "\n",
        "        # TODO: Add a head to project to the amount of output classes you have\n",
        "        self.ln_final = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # TODO: Implement\n",
        "        x = patchify(x, self.patch_size)\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "        x = self.pos_embed(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.ln_final(x)\n",
        "        x = x.mean(dim=1)\n",
        "        logits = self.head(x)\n",
        "        return logits"
      ],
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "id": "a725f966",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-26T16:37:07.041215Z",
          "start_time": "2026-02-26T16:37:07.036472Z"
        },
        "id": "a725f966"
      },
      "source": [
        "@dataclass(frozen=True)\n",
        "class TrainConfig:\n",
        "    seed: int = 0\n",
        "    batch_size: int = 128\n",
        "    epochs: int = 3\n",
        "    lr: float = 3e-4\n",
        "    weight_decay: float = 0.01\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # set \"cuda\" if available"
      ],
      "outputs": [],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "id": "0db652ad",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-26T16:37:07.292703Z",
          "start_time": "2026-02-26T16:37:07.285863Z"
        },
        "id": "0db652ad"
      },
      "source": [
        "def train_one_run(\n",
        "    mlp_kind: str,\n",
        "    model: nn.Module,\n",
        "    train_loader: DataLoader,\n",
        "    test_loader: DataLoader,\n",
        "    cfg: TrainConfig,\n",
        ") -> dict:\n",
        "    model.to(cfg.device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    train_losses: list[float] = []\n",
        "    test_accs: list[float] = []\n",
        "\n",
        "    for epoch in range(cfg.epochs):\n",
        "\n",
        "        # Train loop\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for i, (xb, yb) in enumerate(train_loader):\n",
        "            xb = xb.to(cfg.device)\n",
        "            yb = yb.to(cfg.device)\n",
        "\n",
        "            logits = model(xb)\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            loss = criterion(logits, yb) # TODO: Your criterion\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        avg_training_loss = epoch_loss / len(train_loader)\n",
        "        train_losses.append(avg_training_loss)\n",
        "\n",
        "        # Evaluation loop NOTE: Should be no need to change this\n",
        "        model.eval()\n",
        "        correct = 0.0\n",
        "        total = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in test_loader:\n",
        "                xb = xb.to(cfg.device)\n",
        "                yb = yb.to(cfg.device)\n",
        "                logits = model(xb)\n",
        "                correct += (logits.argmax(dim=-1) == yb).float().sum().item()\n",
        "                total += yb.numel()\n",
        "\n",
        "        test_accs.append(correct / total)\n",
        "        print(f\"[{mlp_kind}] epoch {epoch+1}/{cfg.epochs} | test acc: {test_accs[-1]:.4f}\")\n",
        "\n",
        "        num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return {\n",
        "            \"mlp_kind\": mlp_kind,\n",
        "            \"train_losses\": train_losses,\n",
        "            \"test_accs\": test_accs,\n",
        "            \"final_acc\": test_accs[-1],\n",
        "            \"best_acc\": max(test_accs),\n",
        "            \"num_params\": num_params\n",
        "        }"
      ],
      "outputs": [],
      "execution_count": 38
    },
    {
      "cell_type": "code",
      "id": "ded76097",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2026-02-26T16:41:26.954686Z",
          "start_time": "2026-02-26T16:37:08.146129Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ded76097",
        "outputId": "45915855-8a6f-490a-e198-228f2bd42f8f"
      },
      "source": [
        "cfg = TrainConfig(seed=0, batch_size=128, epochs=5, lr=3e-4, weight_decay=0.01, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tfm = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "train_ds = datasets.MNIST(root=\"./data\", train=True, download=True, transform=tfm)\n",
        "test_ds = datasets.MNIST(root=\"./data\", train=False, download=True, transform=tfm)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# Tiny model example. TODO: You're welcome to experiment with these parameters\n",
        "patch_size = 4\n",
        "d_model = 64\n",
        "n_heads = 4\n",
        "n_layers = 2\n",
        "d_ff = 256\n",
        "dropout = 0.1\n",
        "\n",
        "runs = [\"baseline\", \"Bilinear\", \"GEGLU\", \"SwiGLU\"] # TODO: Name your runs\n",
        "results = []\n",
        "\n",
        "for kind in runs:\n",
        "    model = TinyViT(\n",
        "        patch_size=patch_size,\n",
        "        d_model=d_model,\n",
        "        n_heads=n_heads,\n",
        "        n_layers=n_layers,\n",
        "        d_ff=d_ff,\n",
        "        dropout=dropout,\n",
        "        mlp_kind=kind,\n",
        "    )\n",
        "    # TODO: print anything you might want here\n",
        "    print(f\"\\nRun: {kind} | \" )\n",
        "    out = train_one_run(kind, model, train_loader, test_loader, cfg)\n",
        "    results.append(out)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run: baseline | \n",
            "[baseline] epoch 1/5 | test acc: 0.8905\n",
            "[baseline] epoch 2/5 | test acc: 0.9363\n",
            "[baseline] epoch 3/5 | test acc: 0.9532\n",
            "[baseline] epoch 4/5 | test acc: 0.9574\n",
            "[baseline] epoch 5/5 | test acc: 0.9584\n",
            "\n",
            "Run: Bilinear | \n",
            "[Bilinear] epoch 1/5 | test acc: 0.9238\n",
            "[Bilinear] epoch 2/5 | test acc: 0.9529\n",
            "[Bilinear] epoch 3/5 | test acc: 0.9577\n",
            "[Bilinear] epoch 4/5 | test acc: 0.9672\n",
            "[Bilinear] epoch 5/5 | test acc: 0.9702\n",
            "\n",
            "Run: GEGLU | \n",
            "[GEGLU] epoch 1/5 | test acc: 0.9251\n",
            "[GEGLU] epoch 2/5 | test acc: 0.9546\n",
            "[GEGLU] epoch 3/5 | test acc: 0.9646\n",
            "[GEGLU] epoch 4/5 | test acc: 0.9698\n",
            "[GEGLU] epoch 5/5 | test acc: 0.9726\n",
            "\n",
            "Run: SwiGLU | \n",
            "[SwiGLU] epoch 1/5 | test acc: 0.9225\n",
            "[SwiGLU] epoch 2/5 | test acc: 0.9527\n",
            "[SwiGLU] epoch 3/5 | test acc: 0.9656\n",
            "[SwiGLU] epoch 4/5 | test acc: 0.9687\n",
            "[SwiGLU] epoch 5/5 | test acc: 0.9735\n"
          ]
        }
      ],
      "execution_count": 40
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}